{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7349e23-0cf5-46f7-9d31-d446b1004693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Mon Apr 29 16:15:46 2024\n",
    "\n",
    "@author: Michaela ALksne\n",
    "\n",
    "Script to train a resnet-18 CNN to classify A and B calls in 30 second spectrograms\n",
    "sets model and spectrogram parameters and connects to wandB so user can monitor training progress\n",
    "\n",
    "Model parameters: \n",
    "    - multi-target model: 3 labels per sample\n",
    "    - classification with ResampleLoss function\n",
    "    - weights pretrained on ImageNet\n",
    "    - learning rate = 0.001\n",
    "    - cooling factor = 0.3 (decreases learning rate by multiplying 0.001*3 every ten epochs)\n",
    "    - epochs = 12 \n",
    "    - batch_size = 12\n",
    "\n",
    "Spectrogram parameters:\n",
    "    - 30 second windows\n",
    "    - 3200 Hz(samples/second) sampling rate \n",
    "    - 3200 point-FFT which results in 1 Hz bins\n",
    "    - 90 % overlap (or 1400 samples), resulting in 100 ms bins\n",
    "    - 1600 Hamming window samples. A Hamming window is used to smooth the signal and reduce spectral leakage/artifacts for the FFT. \n",
    "    - minimum frequency: 10 Hz\n",
    "    - maximum frequency: 150 Hz\n",
    "    \n",
    "Spectrogram augmentations: \n",
    "    - frequency_mask: adds random horizontal bars over image\n",
    "    - time_mask: adds random vertical bars over the image\n",
    "    - add_noise: adds random Gaussian noise to image \n",
    "    \n",
    "Notes for user:\n",
    "batch_size – number of training files to load/process before re-calculating the loss function and backpropagation\n",
    "num_workers – parallelization (ie, cores or cpus)\n",
    "log_interval – interval in epochs to evaluate model with validation dataset and print metrics to the log\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d190f873-1b95-4588-964b-782789eef8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opensoundscape\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import librosa\n",
    "import torch\n",
    "import wandb\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956b857e-5a56-483f-a39b-659209b46c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # read in train and validation dataframes\n",
    "train_clips = pd.read_csv('../../data/processed/train.csv') # point to csv files\n",
    "val_clips = pd.read_csv('../../data/processed/validation.csv') # point to csv files\n",
    "print(train_clips.sum()) \n",
    "print(val_clips.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd9b86b-ebbd-463a-a531-12ffacdecc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify relative filepaths \n",
    "data_path = \"YOUR\\DATA\\PATH\\HERE\" # copy and paste the path to your wav files\n",
    "train_clips['file'] = train_clips.file.str.replace(\"..\\\\..\\\\data\\\\raw\\\\\", data_path)\n",
    "val_clips['file'] = val_clips.file.str.replace(\"..\\\\..\\\\data\\\\raw\\\\\", data_path)\n",
    "train_clips.set_index(['file', 'start_time', 'end_time'], inplace=True) \n",
    "val_clips.set_index(['file', 'start_time', 'end_time'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4032d674-4f7e-4394-a4b9-d6f23d16645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "calls_of_interest = [\"A NE Pacific\", \"B NE Pacific\"] #define the calls for CNN\n",
    "model = opensoundscape.CNN('resnet18',classes=calls_of_interest,sample_duration=30.0, single_target=False) # create a CNN object designed to recognize 30-second samples\n",
    "opensoundscape.ml.cnn.use_resample_loss(model) # loss function for mult-target classification\n",
    "\n",
    "# moodify model preprocessing for making spectrograms \n",
    "model.preprocessor.pipeline.to_spec.params.window_type = 'hamming'\n",
    "model.preprocessor.pipeline.to_spec.params.window_samples = 1600 \n",
    "model.preprocessor.pipeline.to_spec.params.overlap_samples = 1400 \n",
    "model.preprocessor.pipeline.to_spec.params.fft_size = 3200 \n",
    "model.preprocessor.pipeline.to_spec.params.decibel_limits = (-120,150)\n",
    "model.preprocessor.pipeline.to_spec.params.scaling = 'density'\n",
    "model.preprocessor.pipeline.bandpass.params.min_f = 10\n",
    "model.preprocessor.pipeline.bandpass.params.max_f = 150\n",
    "model.preprocessor.pipeline.frequency_mask.bypass = True\n",
    "model.preprocessor.pipeline.time_mask.set(max_width = 0.1, max_masks=5) #adds vertical lines as data augmentation\n",
    "model.preprocessor.pipeline.add_noise.set(std=0.1) #adds guassian distributed white noise\n",
    "model.preprocessor.pipeline.random_affine.bypass=True\n",
    "model.optimizer_params['lr']=0.001\n",
    "model.lr_cooling_factor = 0.3 \n",
    "model.wandb_logging['n_preview_samples']=100 # number of samples to look at in wandB\n",
    "\n",
    "model.train(\n",
    "    train_clips, \n",
    "    val_clips, \n",
    "    epochs = 12, \n",
    "    batch_size= 128, \n",
    "    log_interval=1, #log progress every 1 batches\n",
    "    num_workers = 12, \n",
    "    save_interval = 1, #save checkpoint every 1 epoch\n",
    "    save_path = '../../models' #location to save checkpoints (epochs)\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
