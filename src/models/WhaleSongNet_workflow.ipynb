{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63d4a57-8618-49e7-a851-2f4650ccb808",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Mon Apr 29 16:15:46 2024\n",
    "\n",
    "@author: Michaela ALksne\n",
    "\n",
    "Detailed workflow for training and testing WhaleSongNet:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92128a0-b77c-4081-abd8-35ec0b8ebcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Mon Apr 29 16:15:46 2024\n",
    "\n",
    "@author: Michaela ALksne\n",
    "\n",
    "First module to train a resnet-18 CNN to classify A and B calls in 30 second spectrograms\n",
    "sets model and spectrogram parameters and connects to wandB so user can monitor training progress\n",
    "\n",
    "Model parameters: \n",
    "    - multi-target model: 3 labels per sample\n",
    "    - classification with ResampleLoss function\n",
    "    - weights pretrained on ImageNet\n",
    "    - learning rate = 0.001\n",
    "    - cooling factor = 0.3 (decreases learning rate by multiplying 0.001*3 every ten epochs)\n",
    "    - epochs = 12 \n",
    "    - batch_size = 12\n",
    "\n",
    "Spectrogram parameters:\n",
    "    - 30 second windows\n",
    "    - 3200 Hz(samples/second) sampling rate \n",
    "    - 3200 point-FFT which results in 1 Hz bins\n",
    "    - 90 % overlap (or 1400 samples), resulting in 100 ms bins\n",
    "    - 1600 Hamming window samples. A Hamming window is used to smooth the signal and reduce spectral leakage/artifacts for the FFT. \n",
    "    - minimum frequency: 10 Hz\n",
    "    - maximum frequency: 150 Hz\n",
    "    \n",
    "Spectrogram augmentations: \n",
    "    - frequency_mask: adds random horizontal bars over image\n",
    "    - time_mask: adds random vertical bars over the image\n",
    "    - add_noise: adds random Gaussian noise to image \n",
    "    \n",
    "Notes for user:\n",
    "batch_size – number of training files to load/process before re-calculating the loss function and backpropagation\n",
    "num_workers – parallelization (ie, cores or cpus)\n",
    "log_interval – interval in epochs to evaluate model with validation dataset and print metrics to the log\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8e881c-2f4e-48bd-86c3-7bdc8795e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opensoundscape\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import librosa\n",
    "import torch\n",
    "import wandb\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb223b-eae8-42b4-acad-8e8d77942dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    " # read in train and validation dataframes\n",
    "train_clips = pd.read_csv('../../data/processed/train.csv') # point to csv files\n",
    "val_clips = pd.read_csv('../../data/processed/validation.csv') # point to csv files\n",
    "print(train_clips.sum()) \n",
    "print(val_clips.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199f2658-a4ab-4614-bbf3-1c9a4d47157d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify relative filepaths \n",
    "data_path = \"YOUR\\DATA\\PATH\\HERE\" # copy and paste the path to your wav files\n",
    "train_clips['file'] = train_clips.file.str.replace(\"..\\\\..\\\\data\\\\raw\\\\\", data_path)\n",
    "val_clips['file'] = val_clips.file.str.replace(\"..\\\\..\\\\data\\\\raw\\\\\", data_path)\n",
    "train_clips.set_index(['file', 'start_time', 'end_time'], inplace=True) \n",
    "val_clips.set_index(['file', 'start_time', 'end_time'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9130e7-9265-4a3b-87a8-6f7a5690fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN MODEL\n",
    "calls_of_interest = [\"A NE Pacific\", \"B NE Pacific\"] #define the calls for CNN\n",
    "model = opensoundscape.CNN('resnet18',classes=calls_of_interest,sample_duration=30.0, single_target=False) # create a CNN object designed to recognize 30-second samples\n",
    "opensoundscape.ml.cnn.use_resample_loss(model) # loss function for mult-target classification\n",
    "\n",
    "# moodify model preprocessing for making spectrograms \n",
    "model.preprocessor.pipeline.to_spec.params.window_type = 'hamming'\n",
    "model.preprocessor.pipeline.to_spec.params.window_samples = 1600 \n",
    "model.preprocessor.pipeline.to_spec.params.overlap_samples = 1400 \n",
    "model.preprocessor.pipeline.to_spec.params.fft_size = 3200 \n",
    "model.preprocessor.pipeline.to_spec.params.decibel_limits = (-120,150)\n",
    "model.preprocessor.pipeline.to_spec.params.scaling = 'density'\n",
    "model.preprocessor.pipeline.bandpass.params.min_f = 10\n",
    "model.preprocessor.pipeline.bandpass.params.max_f = 150\n",
    "model.preprocessor.pipeline.frequency_mask.bypass = True\n",
    "model.preprocessor.pipeline.time_mask.set(max_width = 0.1, max_masks=5) #adds vertical lines as data augmentation\n",
    "model.preprocessor.pipeline.add_noise.set(std=0.1) #adds guassian distributed white noise\n",
    "model.preprocessor.pipeline.random_affine.bypass=True\n",
    "model.optimizer_params['lr']=0.001\n",
    "model.lr_cooling_factor = 0.3 \n",
    "model.wandb_logging['n_preview_samples']=100 # number of samples to look at in wandB\n",
    "\n",
    "model.train(\n",
    "    train_clips, \n",
    "    val_clips, \n",
    "    epochs = 12, \n",
    "    batch_size= 128, \n",
    "    log_interval=1, #log progress every 1 batches\n",
    "    num_workers = 12, \n",
    "    save_interval = 1, #save checkpoint every 1 epoch\n",
    "    save_path = '../../models' #location to save checkpoints (epochs)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe67329-33b3-434d-926a-70ae99b6b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Mon Jan 22 11:35:46 2024\n",
    "\n",
    "@author: Michaela Alksne\n",
    "\n",
    "Module to use the model for running inference, or predicting on new data.\n",
    "Our model has a \"predict\" function which we can call to predict on a new dataset. \n",
    "In this case, we are predicting on our test data. Therefore we are able to generate preformance metrics as described below. \n",
    "However, if we were predicting on unlabeled data, we would just the use the models predict function and would not be able to plot the preformance metrics.\n",
    "\n",
    "Here we load in our trained model and modify the spectrogram parameters because our test data has a different sampling rate than our training data. This will not effect the model. This is just resizing the images so they match. \n",
    "Our model has a \"predict\" function which we can call to predict on a new dataset. \n",
    "    - We load in our model and our test data and generate predictions. \n",
    "    - Then we join the predictions with the true labels and evaluate model preformance by plotting our precision-recall curve using scikit learn\n",
    "    - We also plot the distribution of our scores for true and false detections. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ed780-6ad6-4936-acd0-f8ec99f7ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE MODEL\n",
    "# read in test dataframes\n",
    "test_clips = pd.read_csv('../../data/processed/test.csv') # point to csv files\n",
    "print(test_clips.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b23d2dd-4e2e-4998-9d4d-7fef9d63d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify relative filepaths \n",
    "data_path = \"YOUR\\DATA\\PATH\\HERE\" # copy and paste the path to your wav files\n",
    "test_clips['file'] = test_clips.file.str.replace(\"..\\\\..\\\\data\\\\raw\\\\\", data_path)\n",
    "test_clips.set_index(['file', 'start_time', 'end_time'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87201bd-9866-415c-991b-6aec1b889cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = opensoundscape.ml.cnn.load_model('../../models/best.model') #read in best model. \n",
    "\n",
    "# moodify model preprocessing for making spectrograms with proper resolution\n",
    "model.preprocessor.pipeline.to_spec.params.window_type = 'hamming' # using hamming window\n",
    "model.preprocessor.pipeline.to_spec.params.window_samples = 1000 # window samples\n",
    "model.preprocessor.pipeline.to_spec.params.overlap_samples = 900 # 90% overlap, for 3200 Fs this means 900 samples, and 0.05 sec bins\n",
    "model.preprocessor.pipeline.to_spec.params.fft_size = 2000 # FFT = Fs, 1 Hz bins\n",
    "model.preprocessor.pipeline.to_spec.params.decibel_limits = (-120,150) # oss preprocessing sets dB limits.\n",
    "\n",
    "# predict \n",
    "test_scores = model.predict(test_clips, num_workers=12,batch_size=128)\n",
    "test_scores.columns = ['pred_A','pred_B']\n",
    "test_all = test_clips.join(test_scores)\n",
    "#save output \n",
    "    \n",
    "## B CALLS ###\n",
    "\n",
    "# plot precision recall curve for B calls\n",
    "precision, recall, thresholds = precision_recall_curve(test_all['B NE Pacific'], test_all['pred_B'])\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recall, precision, color='purple')\n",
    "#add axis labels to plot\n",
    "ax.set_title('Precision-Recall Curve B calls test data')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_xlabel('Recall')\n",
    "plt.show()\n",
    "\n",
    "# plot score distribution B calls \n",
    "B_eval_index = test_all.index[test_all['B NE Pacific']==1]\n",
    "B_eval = test_all.loc[B_eval_index]\n",
    "B_noise_index = test_all.index[test_all['B NE Pacific']==0]\n",
    "B_noise = test_all.loc[B_noise_index]\n",
    "plt.hist(B_noise['pred_B'],bins=40,alpha=0.5,edgecolor='black',color='blue',label='Noise prediction score')\n",
    "plt.hist(B_eval['pred_B'],bins=40,alpha=0.5,edgecolor='black',color='orange',label='B call prediction score')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.semilogy()\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('B call prediction scores test data')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
